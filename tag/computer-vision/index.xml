<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Vision | Paul Liautaud</title><link>https://paulliautaud.github.io/tag/computer-vision/</link><atom:link href="https://paulliautaud.github.io/tag/computer-vision/index.xml" rel="self" type="application/rss+xml"/><description>Computer Vision</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate><image><url>https://paulliautaud.github.io/media/icon_hu5a8bb89b166067dad42de6c6e5349de5_15995_512x512_fill_lanczos_center_3.png</url><title>Computer Vision</title><link>https://paulliautaud.github.io/tag/computer-vision/</link></image><item><title>ViTGAN</title><link>https://paulliautaud.github.io/project/vitgan/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/vitgan/</guid><description>&lt;p>A modular Pytorch Implementation of ViTGAN based on &lt;a href="https://arxiv.org/abs/2107.04589v1" target="_blank" rel="noopener">https://arxiv.org/abs/2107.04589v1&lt;/a>&lt;br>
The goal of this projet is to provide a comprehensive framework to experiment with the ViTGAN architechture.&lt;/p>
&lt;h2 id="getting-started">Getting Started&lt;/h2>
&lt;p>The main file on the provided code contains a simple example using MNIST, which can be trained relatively quickly.
All hyper parameters of the model are specified in the &lt;em>.json&lt;/em> files.
More details are posted on the project link.&lt;/p>
&lt;h2 id="contributors">Contributors&lt;/h2>
&lt;p>&lt;a href="https://github.com/2ailesB" target="_blank" rel="noopener">Lise Le Boudec&lt;/a>, &lt;a href="https://github.com/Nicolivain" target="_blank" rel="noopener">Nicolas Olivain&lt;/a>, &lt;a href="https://github.com/paulliautaud" target="_blank" rel="noopener">Paul Liautaud&lt;/a>&lt;/p>
&lt;h2 id="key-references">Key References&lt;/h2>
&lt;p>&lt;a id="1" href="https://arxiv.org/abs/2107.04589">[1]&lt;/a> ViTGAN: Training GANs with Vision Transformers, Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu. Jul-2021&lt;br>
&lt;a id="2" href="https://arxiv.org/abs/2010.11929">[2]&lt;/a> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby Oct-2020&lt;br>
&lt;a id="3" href="https://arxiv.org/abs/2006.04710">[3]&lt;/a> The Lipschitz Constant of Self-Attention, Hyunjik Kim, George Papamakarios, Andriy Mnih Jun-2020&lt;br>
&lt;a id="4" href="https://arxiv.org/abs/1706.03762">[4]&lt;/a> Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin Jun-2017&lt;br>
&lt;a id="5" href="https://arxiv.org/abs/2107.04589v1">[5]&lt;/a> Generative Adversarial Networks, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio Jun-2014&lt;/p>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;p>One can check this &lt;a href="https://github.com/wilile26811249/ViTGAN" target="_blank" rel="noopener">repository&lt;/a> for a more minimalistic implementation.&lt;/p>
&lt;p>Help us with a
&lt;i class="fas fa-star pr-1 fa-fw">&lt;/i> on our &lt;a href="https://github.com/Nicolivain/ViTGAN" target="_blank" rel="noopener">repository&lt;/a>.&lt;/p></description></item></channel></rss>