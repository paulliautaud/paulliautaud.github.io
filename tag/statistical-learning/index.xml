<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistical Learning | Paul Liautaud</title><link>https://paulliautaud.github.io/tag/statistical-learning/</link><atom:link href="https://paulliautaud.github.io/tag/statistical-learning/index.xml" rel="self" type="application/rss+xml"/><description>Statistical Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate><image><url>https://paulliautaud.github.io/media/icon_hu5a8bb89b166067dad42de6c6e5349de5_15995_512x512_fill_lanczos_center_3.png</url><title>Statistical Learning</title><link>https://paulliautaud.github.io/tag/statistical-learning/</link></image><item><title>Gradient Boosting</title><link>https://paulliautaud.github.io/project/gradient_boosting/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/gradient_boosting/</guid><description>&lt;p>Le Boosting est une méthode d’apprentissage supervisé consistant à bâtir une prédiction fiable en agrégeant les réponses d’apprenants de base, c’est-à-dire d’estimateurs tout juste meilleurs que le hasard. Cette famille d’algorithmes de machine learning construit séquentiellement des apprenants de base, encore appelés faibles ou weak learners. A chaque itération, le nouvel estimateur favorise son apprentissage sur les erreurs du précédent et s’y ajoute pour finalement obtenir un strong learner. Cette méthode a été particulièrement reconnue avec l’algorithme Adaboost (FREUND, SHAPIRE, 1996). Aujourd’hui encore de nombreux challenges sont remportés par des méthodes similaires comme XGBoost/LightGBM (FELLOUS, 2019) réputées puissantes tant sur des modèles de régression que de classification. Le Gradient Boosting proposé par FRIEDMAN (1999 et 2002) est une interprétation du Boosting comme une descente de gradient dans un espace fonctionnel appliquée à un problème d’optimisation dont la fonction objectif est l’erreur en espérance. Cette observation permet d’appliquer la méthode pour un large choix de fonctions de perte comme par exemple l’erreur quadratique en régression ou la fonction logit en classification. Considérons un exemple de weak learner qui constituera notre apprenant de base tout le long de ce cours : l’arbre décisionnel. S’il est peu profond, il est particulièrement simple à mettre en oeuvre et interprétable, en revanche sa pertinence est faible. Comment améliorer ses performances? Augmenter le nombre de noeuds est peu concluant car l’arbre souffre alors d’une variance trop forte. Les méthodes d’agrégation proposent différentes solutions effectives, en générant de multiples versions d’un arbre avant de les combiner. Disposant d’un unique jeu de données, il est alors nécessaire de le perturber pour obtenir des arbres dont les réponses sont différentes mais cohérentes. C’est ce que BREIMAN dénomme perturbing and combining (voir [1]). Dans cet esprit, le Bagging construit chaque arbre à partir d’un échantillon obtenu par bootstrapping : tirage uniforme et avec remise de même taille que le jeu d’entraînement. L’algorithme renvoie la moyenne de la collection d’arbres, estimateur plus robuste, sa variance étant diminuée. Les Forêts aléatoires, procèdent selon le même principe mais les phases d’apprentissage sont réalisées sur des souséchantillons sans remise. La dépendance entre chaque arbre est alors plus faible et par conséquent la variance de leur moyenne également et la réponse plus fiable encore. Venons en au Boosting connu pour surpasser les performances du Bagging et des Forêts alétoires. Comme chaque nouvel arbre apprend des erreurs du précédent et s’y agrège, on peut voir cela comme une diminution du biais. La variance est également améliorée par cet algorithme. Dans [2], il nous est introduit l’algorithme Stochastic Gradient Boosting en injectant de l’aléa par sous échantillonage dans le Grandient Boosting. Les résultats de prédictions peuvent gagner en précision et le coût de calcul est allégé.&lt;/p>
&lt;p>On pourra notamment se référer à ce &lt;a href="https://colab.research.google.com/drive/1JtKMDbNEugHlBmCpFDAPcOk3zs8e2MGa?usp=sharing" target="_blank" rel="noopener">notebook&lt;/a>.&lt;/p>
&lt;h1 id="key-references">Key references&lt;/h1>
&lt;p>&lt;a id="1" href="https://www.researchgate.net/publication/2422419_Bias_Variance_And_Arcing_Classifiers">[1]&lt;/a> Breiman, Leo. (2000). Bias, Variance , And Arcing Classifiers. Technical Report 460, Statistics Department, University of California &lt;br>
&lt;a id="2" href="https://www.researchgate.net/publication/222573328_Stochastic_Gradient_Boosting">[2]&lt;/a> Friedman, Jerome. (2002). Stochastic Gradient Boosting. Computational Statistics &amp;amp; Data Analysis.&lt;/p></description></item><item><title>Kernel Random Forest</title><link>https://paulliautaud.github.io/project/kerf/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/kerf/</guid><description>&lt;h1 id="kerf">KeRF&lt;/h1>
&lt;p>Les forêts aléatoires introduites par Leo Breiman au début des années 2000 sont une méthode de classification et de régression par apprentissage supervisé. L&amp;rsquo;approche repose sur un principe simple mais puissant, &amp;ldquo;diviser pour mieux régner&amp;rdquo; : faire plusieurs sous-échantillonnages des données, construire un arbre de décision pour chaque sous-ensemble selon un paramètre aléatoire, agréger les réponses pour obtenir la prédiction finale. Cette stratégie affiche d&amp;rsquo;excellents résultats dans divers domaines appliqués, pour en nommer quelques-uns : bio-informatique, économétrie ou encore reconnaissance d&amp;rsquo;objets 3D. La robustesse des forêts aléatoires dans des problèmes de très grande dimension associée à leur simplicité pratique (peu de paramètres sont à ajuster) en ont fait une méthode populaire. Ce succès contraste avec le peu de résultats théoriques présents dans la littérature. Les forêts demeurent une question mathématique ouverte, ce sont des objets complexes à analyser. Dans le cadre d&amp;rsquo;un problème de régression, un objectif peut être d&amp;rsquo;établir le lien entre les forêts aléatoires et des estimateurs à noyau obtenus après une légère modification de leur définition. Cette approche permet d&amp;rsquo;obtenir de nouveaux estimateurs nommés KeRF (&lt;em>Kernel Random Forest&lt;/em>), qui satisfont une certaine proximité aux forêts aléatoires (dans certains cas), tout en ouvrant des perspectives d&amp;rsquo;analyses mathématiques plus profondes.&lt;/p>
&lt;h1 id="xp">XP&lt;/h1>
&lt;p>Pour cette section, on pourra notamment se référer à ce &lt;a href="https://colab.research.google.com/drive/1zsIzj7po_NR8CnAsaBhdP--49b3yIdTj?usp=sharing" target="_blank" rel="noopener">notebook&lt;/a>.&lt;/p>
&lt;p>Une comparaison entre KeRF centré, RF centré et RF Breiman évalués sur 2 modèles :&lt;/p>
&lt;ul>
&lt;li>Modèle 1 :&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/90805180/148874839-3360690f-7d34-49d6-b31c-f504a5dd5516.jpg" alt="Model_1" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;ul>
&lt;li>Modèle 2 :&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/90805180/148874841-38b3f2ad-19db-43e9-aa90-5be27eb57b84.jpg" alt="Model_2" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>On peut également représenter 4 arbres centrés opérant sur un jeu de données indépendantes et identiquement distribuées selon une loi uniforme $\mathcal{U}([0;1]^2)$ :&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/90805180/148701851-b73ff73e-f9a6-4506-87fb-c3e3bc601437.jpg" alt="Pathos_example" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Contributions :&lt;/em> Laure Ferraris, Paul Liautaud&lt;/p></description></item></channel></rss>