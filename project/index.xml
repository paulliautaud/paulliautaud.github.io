<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Paul Liautaud</title><link>https://paulliautaud.github.io/project/</link><atom:link href="https://paulliautaud.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate><image><url>https://paulliautaud.github.io/media/icon_hu5a8bb89b166067dad42de6c6e5349de5_15995_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://paulliautaud.github.io/project/</link></image><item><title>Gradient Boosting</title><link>https://paulliautaud.github.io/project/gradient_boosting/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/gradient_boosting/</guid><description>&lt;p>Le Boosting est une méthode d’apprentissage supervisé consistant à bâtir une prédiction fiable en agrégeant les réponses d’apprenants de base, c’est-à-dire d’estimateurs tout juste meilleurs que le hasard. Cette famille d’algorithmes de machine learning construit séquentiellement des apprenants de base, encore appelés faibles ou weak learners. A chaque itération, le nouvel estimateur favorise son apprentissage sur les erreurs du précédent et s’y ajoute pour finalement obtenir un strong learner. Cette méthode a été particulièrement reconnue avec l’algorithme Adaboost (FREUND, SHAPIRE, 1996). Aujourd’hui encore de nombreux challenges sont remportés par des méthodes similaires comme XGBoost/LightGBM (FELLOUS, 2019) réputées puissantes tant sur des modèles de régression que de classification. Le Gradient Boosting proposé par FRIEDMAN (1999 et 2002) est une interprétation du Boosting comme une descente de gradient dans un espace fonctionnel appliquée à un problème d’optimisation dont la fonction objectif est l’erreur en espérance. Cette observation permet d’appliquer la méthode pour un large choix de fonctions de perte comme par exemple l’erreur quadratique en régression ou la fonction logit en classification. Considérons un exemple de weak learner qui constituera notre apprenant de base tout le long de ce cours : l’arbre décisionnel. S’il est peu profond, il est particulièrement simple à mettre en oeuvre et interprétable, en revanche sa pertinence est faible. Comment améliorer ses performances? Augmenter le nombre de noeuds est peu concluant car l’arbre souffre alors d’une variance trop forte. Les méthodes d’agrégation proposent différentes solutions effectives, en générant de multiples versions d’un arbre avant de les combiner. Disposant d’un unique jeu de données, il est alors nécessaire de le perturber pour obtenir des arbres dont les réponses sont différentes mais cohérentes. C’est ce que BREIMAN dénomme perturbing and combining (voir [1]). Dans cet esprit, le Bagging construit chaque arbre à partir d’un échantillon obtenu par bootstrapping : tirage uniforme et avec remise de même taille que le jeu d’entraînement. L’algorithme renvoie la moyenne de la collection d’arbres, estimateur plus robuste, sa variance étant diminuée. Les Forêts aléatoires, procèdent selon le même principe mais les phases d’apprentissage sont réalisées sur des souséchantillons sans remise. La dépendance entre chaque arbre est alors plus faible et par conséquent la variance de leur moyenne également et la réponse plus fiable encore. Venons en au Boosting connu pour surpasser les performances du Bagging et des Forêts alétoires. Comme chaque nouvel arbre apprend des erreurs du précédent et s’y agrège, on peut voir cela comme une diminution du biais. La variance est également améliorée par cet algorithme. Dans [2], il nous est introduit l’algorithme Stochastic Gradient Boosting en injectant de l’aléa par sous échantillonage dans le Grandient Boosting. Les résultats de prédictions peuvent gagner en précision et le coût de calcul est allégé.&lt;/p>
&lt;p>On pourra notamment se référer à ce &lt;a href="https://colab.research.google.com/drive/1JtKMDbNEugHlBmCpFDAPcOk3zs8e2MGa?usp=sharing" target="_blank" rel="noopener">notebook&lt;/a>.&lt;/p>
&lt;h1 id="key-references">Key references&lt;/h1>
&lt;p>&lt;a id="1" href="https://www.researchgate.net/publication/2422419_Bias_Variance_And_Arcing_Classifiers">[1]&lt;/a> Breiman, Leo. (2000). Bias, Variance , And Arcing Classifiers. Technical Report 460, Statistics Department, University of California &lt;br>
&lt;a id="2" href="https://www.researchgate.net/publication/222573328_Stochastic_Gradient_Boosting">[2]&lt;/a> Friedman, Jerome. (2002). Stochastic Gradient Boosting. Computational Statistics &amp;amp; Data Analysis.&lt;/p></description></item><item><title>ViTGAN</title><link>https://paulliautaud.github.io/project/vitgan/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/vitgan/</guid><description>&lt;p>A modular Pytorch Implementation of ViTGAN based on &lt;a href="https://arxiv.org/abs/2107.04589v1" target="_blank" rel="noopener">https://arxiv.org/abs/2107.04589v1&lt;/a>&lt;br>
The goal of this projet is to provide a comprehensive framework to experiment with the ViTGAN architechture.&lt;/p>
&lt;h2 id="getting-started">Getting Started&lt;/h2>
&lt;p>The main file on the provided code contains a simple example using MNIST, which can be trained relatively quickly.
All hyper parameters of the model are specified in the &lt;em>.json&lt;/em> files.
More details are posted on the project link.&lt;/p>
&lt;h2 id="contributors">Contributors&lt;/h2>
&lt;p>&lt;a href="https://github.com/2ailesB" target="_blank" rel="noopener">Lise Le Boudec&lt;/a>, &lt;a href="https://github.com/Nicolivain" target="_blank" rel="noopener">Nicolas Olivain&lt;/a>, &lt;a href="https://github.com/paulliautaud" target="_blank" rel="noopener">Paul Liautaud&lt;/a>&lt;/p>
&lt;h2 id="key-references">Key References&lt;/h2>
&lt;p>&lt;a id="1" href="https://arxiv.org/abs/2107.04589">[1]&lt;/a> ViTGAN: Training GANs with Vision Transformers, Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu. Jul-2021&lt;br>
&lt;a id="2" href="https://arxiv.org/abs/2010.11929">[2]&lt;/a> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby Oct-2020&lt;br>
&lt;a id="3" href="https://arxiv.org/abs/2006.04710">[3]&lt;/a> The Lipschitz Constant of Self-Attention, Hyunjik Kim, George Papamakarios, Andriy Mnih Jun-2020&lt;br>
&lt;a id="4" href="https://arxiv.org/abs/1706.03762">[4]&lt;/a> Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin Jun-2017&lt;br>
&lt;a id="5" href="https://arxiv.org/abs/2107.04589v1">[5]&lt;/a> Generative Adversarial Networks, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio Jun-2014&lt;/p>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;p>One can check this &lt;a href="https://github.com/wilile26811249/ViTGAN" target="_blank" rel="noopener">repository&lt;/a> for a more minimalistic implementation.&lt;/p>
&lt;p>Help us with a
&lt;i class="fas fa-star pr-1 fa-fw">&lt;/i> on our &lt;a href="https://github.com/Nicolivain/ViTGAN" target="_blank" rel="noopener">repository&lt;/a>.&lt;/p></description></item><item><title>Kernel Random Forest</title><link>https://paulliautaud.github.io/project/kerf/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/kerf/</guid><description>&lt;h1 id="kerf">KeRF&lt;/h1>
&lt;p>Les forêts aléatoires introduites par Leo Breiman au début des années 2000 sont une méthode de classification et de régression par apprentissage supervisé. L&amp;rsquo;approche repose sur un principe simple mais puissant, &amp;ldquo;diviser pour mieux régner&amp;rdquo; : faire plusieurs sous-échantillonnages des données, construire un arbre de décision pour chaque sous-ensemble selon un paramètre aléatoire, agréger les réponses pour obtenir la prédiction finale. Cette stratégie affiche d&amp;rsquo;excellents résultats dans divers domaines appliqués, pour en nommer quelques-uns : bio-informatique, économétrie ou encore reconnaissance d&amp;rsquo;objets 3D. La robustesse des forêts aléatoires dans des problèmes de très grande dimension associée à leur simplicité pratique (peu de paramètres sont à ajuster) en ont fait une méthode populaire. Ce succès contraste avec le peu de résultats théoriques présents dans la littérature. Les forêts demeurent une question mathématique ouverte, ce sont des objets complexes à analyser. Dans le cadre d&amp;rsquo;un problème de régression, un objectif peut être d&amp;rsquo;établir le lien entre les forêts aléatoires et des estimateurs à noyau obtenus après une légère modification de leur définition. Cette approche permet d&amp;rsquo;obtenir de nouveaux estimateurs nommés KeRF (&lt;em>Kernel Random Forest&lt;/em>), qui satisfont une certaine proximité aux forêts aléatoires (dans certains cas), tout en ouvrant des perspectives d&amp;rsquo;analyses mathématiques plus profondes.&lt;/p>
&lt;h1 id="xp">XP&lt;/h1>
&lt;p>Pour cette section, on pourra notamment se référer à ce &lt;a href="https://colab.research.google.com/drive/1zsIzj7po_NR8CnAsaBhdP--49b3yIdTj?usp=sharing" target="_blank" rel="noopener">notebook&lt;/a>.&lt;/p>
&lt;p>Une comparaison entre KeRF centré, RF centré et RF Breiman évalués sur 2 modèles :&lt;/p>
&lt;ul>
&lt;li>Modèle 1 :&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/90805180/148874839-3360690f-7d34-49d6-b31c-f504a5dd5516.jpg" alt="Model_1" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;ul>
&lt;li>Modèle 2 :&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/90805180/148874841-38b3f2ad-19db-43e9-aa90-5be27eb57b84.jpg" alt="Model_2" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>On peut également représenter 4 arbres centrés opérant sur un jeu de données indépendantes et identiquement distribuées selon une loi uniforme $\mathcal{U}([0;1]^2)$ :&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/90805180/148701851-b73ff73e-f9a6-4506-87fb-c3e3bc601437.jpg" alt="Pathos_example" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>&lt;em>Contributions :&lt;/em> Laure Ferraris, Paul Liautaud&lt;/p></description></item><item><title>Brownian Motion</title><link>https://paulliautaud.github.io/project/brownian_motion/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://paulliautaud.github.io/project/brownian_motion/</guid><description/></item></channel></rss>